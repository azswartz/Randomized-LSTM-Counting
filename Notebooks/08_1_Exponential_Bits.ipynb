{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08.1-Exponential-Bits.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI4mQnD8jd1z",
        "colab_type": "text"
      },
      "source": [
        "# 08.1 Exponential Bits\n",
        "\n",
        "I'm having two layers of LSTMs. The first one takes the bits and outputs something with an exponential activation. The second one takes the modified bits and the string and outputs the sum. I'm doing this because I don't think the network can manage to exponentiate for 1/2^x on its own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajix1a-JSMYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGRIv7E4SScS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My\\ Drive/CS281\\ Final\\ Project"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYKWXcycBRSc",
        "colab_type": "text"
      },
      "source": [
        "## Package Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19LqsST4SRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WsCo7B6jF3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = 'cuda'\n",
        "\n",
        "from math import isclose\n",
        "from time import time\n",
        "import pickle\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ0QdpRENO3R",
        "colab_type": "text"
      },
      "source": [
        "## Manual Counter Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLaPrg3GN9QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def morris_count(seq):\n",
        "    '''seq[:,0] is the sequence. seq[:,1] is the random floats.'''\n",
        "    x = 0\n",
        "    for i in seq:\n",
        "        if i[0] == 1 and i[1]*(2**x)<=1:\n",
        "            x += 1\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UKlKArUBTPJ",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U1VtUxOz7f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Counter():\n",
        "    def __init__(self, hidden):\n",
        "        '''hidden is the number of hidden variables to use per cell'''\n",
        "        self.hidden = hidden\n",
        "\n",
        "        self.bits_lstm = nn.LSTM(hidden_size=hidden, batch_first=True, input_size=1).double().cuda() \n",
        "        self.bits_dense = torch.randn([hidden,1], dtype=float, device=device, requires_grad=True) \n",
        "        self.string_lstm = nn.LSTM(hidden_size=hidden, batch_first=True, input_size=2).double().cuda() \n",
        "        self.string_dense = torch.randn([hidden,1], dtype=float, device=device, requires_grad=True) \n",
        "\n",
        "        params = list(self.bits_lstm.parameters()) + list(self.string_lstm.parameters())\n",
        "        params.append(self.bits_dense)\n",
        "        params.append(self.string_dense)\n",
        "        self.optimizer = optim.Adam(params)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_sequence(seq):\n",
        "        '''converts a set of sequences with the same length from array or numpy into a correctly formatted tensor.'''\n",
        "        seq = torch.tensor(seq, device=device).double()\n",
        "        seq = seq.reshape([len(seq), -1, 1])\n",
        "        return seq\n",
        "\n",
        "    def predict(self, sequence):\n",
        "        '''takes a tensor, predicts the sum of the tensor, and compares to the target sum of the tensor. \n",
        "        Returns the loss and the predicted sum'''\n",
        "\n",
        "        floats = torch.rand_like(sequence)\n",
        "        bits,_ = self.bits_lstm(floats)\n",
        "        bits = bits @ self.bits_dense\n",
        "        bits = torch.exp(bits)\n",
        "        combined = torch.cat([sequence, bits],2)\n",
        "        out, _ = self.string_lstm(combined)\n",
        "        add = torch.sigmoid(out @ self.string_dense)\n",
        "        count = add.sum(1).squeeze(1)\n",
        "\n",
        "        sequence = torch.cat([sequence, floats], 2)\n",
        "        target = self.true_sum(sequence)\n",
        "        loss = (count - target).pow(2)\n",
        "        return loss, count\n",
        "\n",
        "    def predict_multilength(self, sequences):\n",
        "        '''Takes a list of batches of tensors of different length. Predicts on each batch. Sums up the loss. Reduces to a single mean'''\n",
        "        loss = torch.tensor(0, device=device).double()\n",
        "        count= torch.tensor(0, device=device).double()\n",
        "        for s in sequences:\n",
        "            res    = self.predict(s)[0]\n",
        "            count += res.shape[0]\n",
        "            loss  += res.sum()\n",
        "        return loss / count\n",
        "\n",
        "    @staticmethod\n",
        "    def true_sum(sequence):\n",
        "        '''Determines the true sums of a batch of sequences to train against'''\n",
        "        res = []\n",
        "        for seq in sequence:\n",
        "            res.append(morris_count(seq))\n",
        "        res = np.array(res, dtype=float)\n",
        "        return torch.tensor(res, device=device).double()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWFnUlWBqLM",
        "colab_type": "text"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvC9waVe4EGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def round_partition(p):\n",
        "    '''Rounds a partition so that the sum of the partition equals the original sum'''\n",
        "    rounded = np.round(p).astype(int)\n",
        "    ind = len(rounded)-1\n",
        "    while rounded.sum() < p.sum():\n",
        "        rounded[ind] += 1\n",
        "        ind -= 1\n",
        "    return rounded\n",
        "\n",
        "def generate_data(length, total):\n",
        "    counts = np.random.dirichlet((np.arange(length)+1)**2) * total * 0.9\n",
        "    counts = round_partition(counts)\n",
        "\n",
        "    train_set = []\n",
        "    val_set = []\n",
        "    test_set = []\n",
        "    for i in range(1,length+1):\n",
        "        if counts[i-1] == 0:\n",
        "            continue\n",
        "        seqs = np.random.randint(0,2, size=[counts[i-1],i])\n",
        "        seqs = np.unique(seqs, axis=0)[:,:,None]\n",
        "        try:\n",
        "            train, val = train_test_split(seqs, test_size=2/9, shuffle=True)\n",
        "            train = Counter.convert_sequence(train)\n",
        "            val = Counter.convert_sequence(val)\n",
        "            train_set.append(train)\n",
        "            val_set.append(val)\n",
        "        except ValueError:\n",
        "            train = Counter.convert_sequence(seqs)\n",
        "            train_set.append(train)\n",
        "            continue\n",
        "    counts = np.random.dirichlet((np.arange(length, 2*length)+1)**2) * total * 0.1\n",
        "    counts = round_partition(counts)\n",
        "\n",
        "    for i in range(length):\n",
        "        if counts[i] == 0:\n",
        "            continue\n",
        "        seqs = np.random.randint(0,2, size=[counts[i],i+length+1])\n",
        "        seqs = np.unique(seqs, axis=0)[:,:,None]\n",
        "        seqs = Counter.convert_sequence(seqs)\n",
        "        test_set.append(seqs)\n",
        "\n",
        "    return train_set, val_set, test_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80ekLB3rz96d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate all the strings and partition into train and test\n",
        "length = 64\n",
        "hidden = 10\n",
        "depth = 100\n",
        "\n",
        "output_folder = \"Part-8.1-Outputs\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V28nvi7EGArd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train, val, test = generate_data(length,depth)\n",
        "\n",
        "# with open(\"%s/Data.pickle\"%output_folder, \"wb\") as f:\n",
        "#     pickle.dump([train, val, test], f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k82PKEhGETm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"%s/Data.pickle\"%output_folder, \"rb\") as f:\n",
        "    train, val, test = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIQ6ykY8G65T",
        "colab_type": "code",
        "outputId": "1e5ae318-acf8-4cb3-9a0f-033b00ff41c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "trainsize = sum([x.shape[0] for x in train])\n",
        "valsize = sum([x.shape[0] for x in val])\n",
        "testsize = sum([x.shape[0] for x in test])\n",
        "print(trainsize, valsize, testsize)\n",
        "\n",
        "total = trainsize+valsize+testsize\n",
        "print(\"Total:\",total) \n",
        "print(\"Fraction %.3f %.3f %.3f\"%(trainsize/total, valsize/total, testsize/total))\n",
        "\n",
        "print(\"Train    string range: %d-%d\"%(min([x.shape[1] for x in train]), max([x.shape[1] for x in train])))\n",
        "print(\"Validate string range: %d-%d\"%(min([x.shape[1] for x in val]), max([x.shape[1] for x in val])))\n",
        "print(\"Test     string range: %d-%d\"%(min([x.shape[1] for x in test]), max([x.shape[1] for x in test])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61 29 10\n",
            "Total: 100\n",
            "Fraction 0.610 0.290 0.100\n",
            "Train    string range: 23-64\n",
            "Validate string range: 39-64\n",
            "Test     string range: 119-128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO19JkYIXSAU",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsuiwPh729fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train over all the training data\n",
        "model = Counter(hidden=hidden)\n",
        "\n",
        "history = []\n",
        "best = float('inf')\n",
        "patience = 10\n",
        "tol = 0.001\n",
        "count = 0\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1,1000000): \n",
        "    shuffle(train)\n",
        "    shuffle(val)\n",
        "    if epoch % 100 == 0:\n",
        "        train_loss = model.predict_multilength(train).item()\n",
        "        with torch.no_grad():\n",
        "            val_loss = model.predict_multilength(val).item()\n",
        "        history.append([train_loss, val_loss])\n",
        "        print(\"Epoch: %5d. Train Loss: %7.3f. Validation Loss: %7.3f. Elapsed: %7.3f\"%(epoch, train_loss, val_loss, (time()-start)/60))\n",
        "        start = time()\n",
        "\n",
        "        if val_loss + tol < best:\n",
        "            best = val_loss\n",
        "            count = 0\n",
        "            torch.save(model, \"%s/Model\"%output_folder)\n",
        "        else:\n",
        "            count += 1\n",
        "        if count >= patience:\n",
        "            break\n",
        "\n",
        "    #take the average loss over all the train data\n",
        "    loss = model.predict_multilength(train)   \n",
        "    #and update\n",
        "    model.optimizer.zero_grad()\n",
        "    loss.backward(retain_graph=True)\n",
        "    model.optimizer.step()\n",
        "\n",
        "# history = np.array(history)\n",
        "# np.save(\"%s/Train-History\"%output_folder, history)\n",
        "# torch.save(model, \"%s/Model\"%output_folder)\n",
        "\n",
        "# #display testing results\n",
        "# loss = model.predict_multilength(test)\n",
        "# print(\"Average Test Loss:\", loss.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azs8MF9xXe4v",
        "colab_type": "text"
      },
      "source": [
        "## Results Evaluation\n",
        "\n",
        "Same deal as in last time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuDksNrpXgGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load(\"%s/Model\"%output_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Muev3LXZFZM",
        "colab_type": "code",
        "outputId": "4b57576b-7382-4f22-f976-bcd8ff49e8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "loss = model.predict_multilength(train)\n",
        "print(\"Average Train Loss:\", loss.item())\n",
        "loss = model.predict_multilength(val)\n",
        "print(\"Average Val   Loss:\", loss.item())\n",
        "loss = model.predict_multilength(test)\n",
        "print(\"Average Test  Loss:\", loss.item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.5699139165760533\n",
            "Average Val   Loss: 0.4611735518111265\n",
            "Average Test  Loss: 2.950724497199972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezqxGcQaZJgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = []\n",
        "pred = []\n",
        "size = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi0ruB2nQCGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for length in range(1000,10000,1000):\n",
        "    depth = np.random.randint(1,10)\n",
        "    seq = np.random.randint(0,2,size=(depth,length,1))\n",
        "    seq = torch.tensor(seq).double().cuda()\n",
        "    l,p = model.predict(seq)\n",
        "    l = l.tolist()\n",
        "    p = p.tolist()\n",
        "    loss.extend(l)\n",
        "    pred.extend(p)\n",
        "    size.extend([length for _ in p])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUROJG-BQDqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true = []\n",
        "for l,p in zip(loss,pred):\n",
        "    t = p - np.sqrt(l)\n",
        "    if not isclose(t, int(t)):\n",
        "        t = p + np.sqrt(l)\n",
        "    true.append(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtTSuBsXQIw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = pd.DataFrame({\"Predicted\":pred, \"Loss\":loss, \"Length\":size, \"Real\":true})\n",
        "res.to_csv(\"%s/Large_testing-8.1\"%output_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI0MHxvYQKFy",
        "colab_type": "code",
        "outputId": "fbe40348-ce0a-42db-f77e-7824322451ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "res = pd.read_csv(\"%s/Large_testing-8.1\"%output_folder)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Length</th>\n",
              "      <th>Real</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.704297</td>\n",
              "      <td>0.496035</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.718734</td>\n",
              "      <td>0.516579</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.677345</td>\n",
              "      <td>0.104106</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.698321</td>\n",
              "      <td>0.487652</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.653426</td>\n",
              "      <td>0.120114</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>693</th>\n",
              "      <td>693</td>\n",
              "      <td>305.911550</td>\n",
              "      <td>86972.822084</td>\n",
              "      <td>9000</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>694</td>\n",
              "      <td>307.059699</td>\n",
              "      <td>87651.345470</td>\n",
              "      <td>9000</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>695</td>\n",
              "      <td>304.642808</td>\n",
              "      <td>85639.812949</td>\n",
              "      <td>9000</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>696</td>\n",
              "      <td>309.954516</td>\n",
              "      <td>87589.075750</td>\n",
              "      <td>9000</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>697</td>\n",
              "      <td>303.787561</td>\n",
              "      <td>86311.131257</td>\n",
              "      <td>9000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>698 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0   Predicted          Loss  Length  Real\n",
              "0             0    0.704297      0.496035       2   0.0\n",
              "1             1    0.718734      0.516579       2   0.0\n",
              "2             2    0.677345      0.104106       2   1.0\n",
              "3             3    0.698321      0.487652       2   0.0\n",
              "4             4    0.653426      0.120114       2   1.0\n",
              "..          ...         ...           ...     ...   ...\n",
              "693         693  305.911550  86972.822084    9000  11.0\n",
              "694         694  307.059699  87651.345470    9000  11.0\n",
              "695         695  304.642808  85639.812949    9000  12.0\n",
              "696         696  309.954516  87589.075750    9000  14.0\n",
              "697         697  303.787561  86311.131257    9000  10.0\n",
              "\n",
              "[698 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4mJkfY5QhU_",
        "colab_type": "code",
        "outputId": "38e96ec4-6ca2-46c6-952a-96ebc7179110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "sns.set()\n",
        "sns.scatterplot(x=\"Real\", y=\"Predicted\", data=res, color='k')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9508471668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEMCAYAAADXiYGSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8v+lyq8WsVGiPQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuG4JLYTQmw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}